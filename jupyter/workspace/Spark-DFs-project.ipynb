{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project\n",
    "- For the project we'll be using OfficeDataProject.csv\n",
    "- Read data from the file in the DF and perform following analytics on it.\n",
    "    - Print the total number of employees in the company\n",
    "    - Print the total number of departments in the company\n",
    "    - Print the department names of the company\n",
    "    - Print the total number of employees in each department\n",
    "    - Print the total number of employees in each state\n",
    "    - Print the total number of employees in each state in each department\n",
    "    - Print the minimum and maximum salaries in each department and sort salaries in ascending order\n",
    "    - Print the names of employees working in NY state under Finance department whose bonuses are greater than the average bonuses of employees in NY state\n",
    "    - Raise the salaries $500 of all employees whose age is greater than 45\n",
    "    - Create DF of all those employees whose age is greater than 45 and save them in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.functions import sum, avg, max, min, mean, count\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark DataFrame Project').getOrCreate()\n",
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/OfficeDataProject.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of employees in the company\n",
    "df.select(df.employee_id).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of departments in the company\n",
    "df.select(df.department).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the department names of the company\n",
    "df.select(df.department).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of employees in each department\n",
    "df.groupBy(df.department).agg(count(df.employee_id).alias(\"total_employees_in_department\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of employees in each state\n",
    "df.groupBy(df.state).agg(count(df.employee_id).alias(\"total_employees_each_sate\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of employees in each state in each department\n",
    "df.groupBy(df.state, df.department).agg(count(df.employee_id).alias(\"total_employees\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum and maximum salaries in each department and sort salaries in ascending order\n",
    "df.groupBy(df.department).agg(min(df.salary).alias('min'), max(df.salary).alias('max')).orderBy(col('min').asc(), col('max').asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of employees working in NY state under Finance department whose bonuses are greater than the average bonuses of employees in NY state\n",
    "df_avg = df.filter(df.state=='NY').groupBy(df.state).agg(mean(df.bonus).alias('avg_bonus')).select(col('avg_bonus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.collect()[0]['avg_bonus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.state == 'NY') & (df.department == 'Finance') & (df.bonus > df_avg.collect()[0]['avg_bonus'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raise the salaries $500 of all employees whose age is greater than 45\n",
    "from pyspark.sql. types import StructType, StructField, StringType, IntegerType\n",
    "def raise_salary_corr_age(age, salary):\n",
    "    if age > 45:\n",
    "        return salary + 500\n",
    "    return salary\n",
    "    \n",
    "RaiseSalaryCorrAge = udf(lambda x, y: raise_salary_corr_age(x, y), IntegerType())\n",
    "    \n",
    "df1 = df.withColumn('salary', RaiseSalaryCorrAge(df.age, df.salary))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DF of all those employees whose age is greater than 45 and save them in a file\n",
    "df_save = df1.filter(df1.age > 45)\n",
    "df_save.write.mode('overwrite').options(header='True').csv('data/output/project_df')\n",
    "df_read = spark.read.options(header='True',inferSchema='True').csv('data/output/project_df')\n",
    "df_read.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
