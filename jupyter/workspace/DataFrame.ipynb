{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Spark DataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/StudentData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql. types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType ([\n",
    "    StructField (\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType (), True),\n",
    "    StructField (\"name\", StringType (), True),\n",
    "    StructField(\"course\", StringType (), True), \n",
    "    StructField(\"roll\", StringType (), True),\n",
    "    StructField(\"marks\", IntegerType (), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header='True').schema(schema).csv('data/StudentData.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName('DF RDD').set(\"spark.ui.port\", \"8081\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# transformation read the file\n",
    "rdd = sc.textFile('data/StudentData.csv')\n",
    "header = rdd.first()\n",
    "rdd1 = rdd.filter(lambda x: x != header).map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = header.split(',')\n",
    "dfRdd = rdd1.toDF(columns)\n",
    "dfRdd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql. types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType ([\n",
    "    StructField (\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType (), True),\n",
    "    StructField (\"name\", StringType (), True),\n",
    "    StructField(\"course\", StringType (), True), \n",
    "    StructField(\"roll\", StringType (), True),\n",
    "    StructField(\"marks\", IntegerType (), True),\n",
    "    StructField(\"email\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda x: [int(x[0]), x[1], x[2], x[3], x[4], int(x[5]), [6]])\n",
    "dfRdd1 = spark.createDataFrame(rdd2, schema=schema)\n",
    "dfRdd.printSchema()\n",
    "dfRdd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRdd.select('age', 'gender').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRdd.select(dfRdd.age, dfRdd.gender).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "dfRdd.select(col('roll'),col('name')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRdd.select(dfRdd.columns[2:5]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('marks', col('marks') + 10)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn('aggregated_marks', col('marks') - 10)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.withColumn('Country', lit('VN'))\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.withColumnRenamed('gender', 'sex')\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col('name').alias('full name')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(((df.course == \"DB\") | (df.course == \"Cloud\")) & (df.marks > 50)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = [\"DB\", \"Cloud\", \"OOP\", \"DSA\"]\n",
    "df.filter(df.course.isin(courses)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.course.startswith(\"D\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.like('%s%e%')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "- For the quiz you'll be using StudentData.csv\n",
    "- Read this file in the DF\n",
    "- Create a new column in the DF for total marks and let the total marks be 120\n",
    "- Create a new column average to calculate the average marks of the student.\n",
    "    - (marks / total marks) * 100\n",
    "- Filter out all those students who have achieved more than 80% marks in OOP course and save it in a new DF.\n",
    "- Filter out all those students who have achieved more than 60% marks in Cloud course and save it in a new DE.\n",
    "- Print the names and marks of all the students from the above DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "spark = SparkSession.builder.appName('Spark DataFrame Quiz').getOrCreate()\n",
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/StudentData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('total marks',lit(120))\n",
    "df = df.withColumn('average',(col('marks')/120)*100)\n",
    "df1 = df.filter((df.course == 'OOP') & (df.average > 80))\n",
    "df2 = df.filter((df.course == 'Cloud') & (df.average > 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select(col('name'), col('marks'), col('course'), col('average')).show(5)\n",
    "df2.select(col('name'), col('marks'), col('course'), col('average')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count, Distinct, Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.course == \"DB\").count ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"gender\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates([\"gender\"]).show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.marks.asc(),df.age.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "- Create a DF, sorted on bonus in ascending order and show it.\n",
    "- Create a DF, sorted on age and salary in descending and ascending order respectively and show it.\n",
    "- Create a DF sorted on age, bonus and salary in descending, descending and ascending order respectively and show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/OfficeData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.bonus.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.age.desc(), df.salary.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.age.desc(),df.bonus.desc(), df.salary.asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('gender').sum('marks').show()\n",
    "df.groupBy('gender').count().show()\n",
    "df.groupBy('gender').max('marks').show()\n",
    "df.groupBy('age').avg('marks').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('age','gender').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, max, min, mean, count\n",
    "df.groupBy(\"course\",\"gender\").agg(count(\"*\").alias(\"total_enrollments\"),\n",
    "                         sum(\"marks\").alias(\"total_marks\"),\n",
    "                         min(\"marks\").alias(\"min_makrs\"),\n",
    "                         max(\"marks\").alias(\"max_makrs\"),\n",
    "                         avg(\"marks\").alias(\"avg_makrs\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dung df.total_enrollments o filter => error => dung col('total_enrollments')\n",
    "df.filter(df.gender == \"Male\").groupBy(\"course\", \"gender\")\\\n",
    "    .agg(count(\"*\")\\\n",
    "    .alias(\"total_enrollments\"))\\\n",
    "    .filter(col(\"total_enrollments\") > 85).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ\n",
    "- For the quiz you'll be using StudentData.csv\n",
    "- Read this file in the DF\n",
    "- Display the total numbers of students enrolled in each course\n",
    "- Display the total number of male and female students enrolled in each course\n",
    "- Display the total marks achieved by each gender in each course\n",
    "- Display the minimum, maximum and average marks achieved in each course by each age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/StudentData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('course').agg(count(\"*\").alias(\"total_enrollments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('course','gender').agg(count(\"*\").alias(\"total_enrollments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('course','gender').agg(sum(\"marks\").alias(\"total_marks\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('course','gender').\\\n",
    "    agg(min(\"marks\"), max('marks'), avg('marks')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ\n",
    "- For the quiz you'll be using WordData. txt\n",
    "- Read this file in the DF\n",
    "- Calculate and show the count of each word present in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName('DF RDD').set(\"spark.ui.port\", \"8081\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# transformation read the file\n",
    "rdd = sc.textFile('data/WordData.txt')\n",
    "rdd1 = rdd.map(lambda x: [x,len(x)])\n",
    "df = rdd1.toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('word').agg(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType ([\n",
    "    StructField(\"value\", StringType(), True),\n",
    "])\n",
    "df = spark.read.option(\"header\", \"false\").schema(schema).csv('data/WordData.txt')\n",
    "df.groupBy('value').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs - user defined funtions\n",
    "- su dung function udf de convert my function thanh 1 function duoc dung nhu select, filter,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "def get_total_salary (salary, bonus):\n",
    "    return salary + bonus\n",
    "\n",
    "\n",
    "totalSalaryUDF = udf(lambda x, y: get_total_salary(x, y), IntegerType())\n",
    "\n",
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/OfficeData.csv')\n",
    "df.withColumn(\"total_salary\", totalSalaryUDF(df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"total_salary\", get_total_salary(df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tai sao khong dung function truc tiep?\n",
    "\n",
    "For example, you can find that if you write:\n",
    "\n",
    "``spark.sql(\"select replaceBlanksWithNulls(column_name) from dataframe\")``\n",
    "\n",
    "does not work if you didn't register the function replaceBlanksWithNulls as a udf. In spark sql we need to know the returned type of the function for the exectuion. Hence, we need to register the custom function as a user-defined function (udf) to be used in spark sql."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ\n",
    "- For the quiz you'll be using OfficeData.csv\n",
    "- Read this file in the DF\n",
    "- Create a new column increment and provide the increment to the employees on the following criteria\n",
    "    - If the employee is in NY state, his increment would be 10% of salary plus 5% of bonus\n",
    "    - If the employee is in CA state, his increment would be 12% of salary plus 3% of bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_increment(state, salary, bonus):\n",
    "    increment = 0\n",
    "    if state == 'NY':\n",
    "        increment = salary*10/100 + bonus*5/100\n",
    "    elif state == 'CA':\n",
    "        increment = salary*12/100 + bonus*3/100\n",
    "    \n",
    "    return increment\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calcIncrement = udf(lambda x, y, z: calc_increment(x, y, z))\n",
    "\n",
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/OfficeData.csv')\n",
    "df.withColumn('increment', calcIncrement(df.state, df.salary, df.bonus)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema='True',header='True',delimiter=',').csv('data/StudentData.csv')\n",
    "rdd = df.rdd\n",
    "# rdd.filter(lambda x: x['age'] == 28).collect()\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select course from Student where gender=\"Male\"').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sql below equal this\n",
    "df.select('course').filter(df.gender == 'Male').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupBy('course','gender').\\\n",
    "    agg(min(\"marks\"), max('marks'), avg('marks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.mode('overwrite').options(header='True').csv('data/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.options(header='True',inferSchema='True').csv('data/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
