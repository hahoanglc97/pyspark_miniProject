{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7a383b",
   "metadata": {},
   "source": [
    "# CDC - Change Data Capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd732eb8",
   "metadata": {},
   "source": [
    "### Import data into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee4b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open('project/Dump.txt', 'r')\n",
    "sqlFile = fd.read()\n",
    "fd.close()\n",
    "\n",
    "sqlCommands = sqlFile.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c46de1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "db_connection = mysql.connector.connect(\n",
    "    host='10.10.0.100',\n",
    "    user=\"root\",\n",
    "    password=\"1\",\n",
    "    database=\"\")\n",
    "db_cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bda3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sqlCommands[-1]\n",
    "for command in sqlCommands:\n",
    "    try:\n",
    "        if command != '':\n",
    "            db_cursor.execute(command)\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Something went wrong: {}\".format(err))\n",
    "db_connection.commit()   \n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b45df",
   "metadata": {},
   "source": [
    "### Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214e364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fb8049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 07:11:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/31 07:11:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('CDC').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c7aad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fldf = spark.read.csv('project/LOAD00000001.csv')\n",
    "fldf = fldf.withColumnRenamed('_c0','id')\\\n",
    ".withColumnRenamed('_c1','FullName')\\\n",
    ".withColumnRenamed('_c2','City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd4a0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fldf.write.mode('overwrite').options(header='True').csv('project/output/finalFile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c4c51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = spark.read.csv('project/20230531-041611011.csv')\n",
    "udf = udf.withColumnRenamed('_c0','action')\\\n",
    ".withColumnRenamed('_c1','id')\\\n",
    ".withColumnRenamed('_c2','FullName')\\\n",
    ".withColumnRenamed('_c3','City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446290c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffdf = spark.read.options(header='True').csv('project/output/finalFile.csv')\n",
    "ffdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02a75cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id', 'FullName', 'City']\n",
    "for row in udf.collect():\n",
    "    if row['action'] == 'U':\n",
    "        ffdf = ffdf.withColumn(\"FullName\", when(ffdf[\"id\"] == row[\"id\"], row[\"FullName\"])\\\n",
    "                               .otherwise(ffdf [\"FullName\"]))\n",
    "        ffdf = ffdf.withColumn(\"City\", when(ffdf[\"id\"] == row[\"id\"], row[\"City\"])\\\n",
    "                               .otherwise(ffdf[\"City\"]))\n",
    "        \n",
    "    if row['action'] == 'I':\n",
    "        insertData = [list(row[1:])]\n",
    "        newdf = spark.createDataFrame(insertData, columns)\n",
    "        ffdf = ffdf.union(newdf)\n",
    "        \n",
    "    if row['action'] == 'D':\n",
    "        ffdf = ffdf.filter(ffdf.id != row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "506587c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2cf5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------------+\n",
      "| id|        FullName|         City|\n",
      "+---+----------------+-------------+\n",
      "|  0|Herman Zimmerman|Oklahoma City|\n",
      "+---+----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ffdf.cache().show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b4e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdf.write.mode('overwrite').options(header='True').csv('project/output/finalFile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6c18982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffdf1 = spark.read.options(header='True').csv('project/output/finalFile.csv')\n",
    "ffdf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757303b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
